{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9c6a55",
   "metadata": {},
   "source": [
    "# Download BioBert, Prepare DataSets For NER and RE Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba394d1",
   "metadata": {
    "gather": {
     "logged": 1652459454627
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using version 1.40.0 of the Azure ML SDK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import azureml\n",
    "import tempfile\n",
    "import azureml.core\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core import Model, Workspace, Environment, Run, Dataset, Datastore, ScriptRunConfig, Experiment, ComputeTarget\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96579322",
   "metadata": {
    "gather": {
     "logged": 1652459457911
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: udacity-capstone-ws\n",
      "Azure region: northeurope\n",
      "Subscription id: fbe09221-d2fa-4355-8174-808a6c0b6925\n",
      "Resource group: udacity-capstone\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a4df9c",
   "metadata": {
    "gather": {
     "logged": 1652459465316
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"workspaceblobstore\",\n",
      "  \"container_name\": \"azureml-blobstore-644b04ba-cd6a-402f-aa72-d1730d303ea8\",\n",
      "  \"account_name\": \"udacitycapston2606602571\",\n",
      "  \"protocol\": \"https\",\n",
      "  \"endpoint\": \"core.windows.net\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b24950",
   "metadata": {
    "gather": {
     "logged": 1652459473738
    }
   },
   "outputs": [],
   "source": [
    "# cpu_cluster = ComputeTarget(workspace=ws, name=\"StandardDS11v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108190d7",
   "metadata": {},
   "source": [
    "### Download And Register Pretrained Biobert Model\n",
    "In this step it's used for tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2726ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3253ea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae65474456741459264c712a9e2cea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ac228890ec41d9ab32fc12d24003d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/467k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b17e0f84cb34aa28cdf213a9a759be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_large = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1\")\n",
    "model_large = AutoModel.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62bd842f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35efb01f716548a0b4f2c408d36b2878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85057c470e5543f599e3019847c5cf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a07a90684b4a45bb61ca626e013842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model_base = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170618fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../biobert-base-cased-v1.1/tokenizer_config.json',\n",
       " '../biobert-base-cased-v1.1/special_tokens_map.json',\n",
       " '../biobert-base-cased-v1.1/vocab.txt',\n",
       " '../biobert-base-cased-v1.1/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large.save_pretrained(\"../biobert-large-cased-v1.1\")\n",
    "tokenizer_base.save_pretrained(\"../biobert-base-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b532251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large.save_pretrained(\"../biobert-large-cased-v1.1\")\n",
    "model_base.save_pretrained(\"../biobert-base-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b11e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model biobert-large-cased-v1.1\n",
      "Name: biobert-large-cased-v1.1\n",
      "Version: 1\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(workspace=ws,\n",
    "                       model_name='biobert-large-cased-v1.1',                # Name of the registered model in your workspace.\n",
    "                       model_path='../biobert-large-cased-v1.1',  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.PYTORCH,  # Framework used to create the model.\n",
    "                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                       description='BioBERT, a biomedical language representation model designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc.',\n",
    "                       tags={'area': 'biobert-large', 'type': 'transformers'})\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3091f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model biobert-base-cased-v1.1\n",
      "Name: biobert-base-cased-v1.1\n",
      "Version: 1\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(workspace=ws,\n",
    "                       model_name='biobert-base-cased-v1.1',                # Name of the registered model in your workspace.\n",
    "                       model_path='../biobert-base-cased-v1.1',  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.PYTORCH,  # Framework used to create the model.\n",
    "                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                       description='BioBERT, a biomedical language representation model designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc.',\n",
    "                       tags={'area': 'biobert-base', 'type': 'transformers'})\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b092e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3400e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"n2c2_2018\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        dataset = ws.datasets[key]\n",
    "        \n",
    "# download the dataset \n",
    "dataset.download(target_path='../', overwrite=False) \n",
    "\n",
    "# mount dataset to the temp directory at `mounted_path`\n",
    "\n",
    "mounted_path = tempfile.mkdtemp()\n",
    "mount_context = dataset.mount(mounted_path)\n",
    "\n",
    "mount_context.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462f2193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../n2c2_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5877e50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ade_split_0_test.json\tade_split_3_train.json\tade_split_7_test.json\r\n",
      "ade_split_0_train.json\tade_split_4_test.json\tade_split_7_train.json\r\n",
      "ade_split_1_test.json\tade_split_4_train.json\tade_split_8_test.json\r\n",
      "ade_split_1_train.json\tade_split_5_test.json\tade_split_8_train.json\r\n",
      "ade_split_2_test.json\tade_split_5_train.json\tade_split_9_test.json\r\n",
      "ade_split_2_train.json\tade_split_6_test.json\tade_split_9_train.json\r\n",
      "ade_split_3_test.json\tade_split_6_train.json\r\n"
     ]
    }
   ],
   "source": [
    "key = \"ade_corpus\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        ade_data = ws.datasets[key]\n",
    "\n",
    "# download the dataset \n",
    "ade_data.download(target_path='../', overwrite=False) \n",
    "\n",
    "# mount dataset to the temp directory at `mounted_path`\n",
    "mounted_path = tempfile.mkdtemp()\n",
    "mount_context = ade_data.mount(mounted_path)\n",
    "\n",
    "mount_context.start()\n",
    "!ls ../ade_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0fc9",
   "metadata": {},
   "source": [
    "### Generate Data For NER Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and activate conda env, install requirements and run in terminal:\n",
    "# !python generate_data.py \\\n",
    "#     --task ner \\\n",
    "#     --input_dir ../n2c2_2018/ \\\n",
    "#     --ade_dir ../ade_corpus/ \\\n",
    "#     --target_dir dataset_ner/ \\\n",
    "#     --max_seq_len 512 \\\n",
    "#     --dev_split 0.1 \\\n",
    "#     --tokenizer biobert-base \\\n",
    "#     --ext txt \\\n",
    "#     --sep \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d385b0",
   "metadata": {},
   "source": [
    "### Generate Data For RE Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python generate_data.py \\\n",
    "#     --task re \\\n",
    "#     --input_dir ../n2c2_2018/ \\\n",
    "#     --ade_dir ../ade_corpus/ \\\n",
    "#     --target_dir dataset_re/ \\\n",
    "#     --max_seq_len 512 \\\n",
    "#     --dev_split 0.1 \\\n",
    "#     --tokenizer biobert-base \\\n",
    "#     --ext tsv \\\n",
    "#     --sep tab "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5293da1",
   "metadata": {},
   "source": [
    "### Register Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1083042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 9 files\n",
      "Uploading ./dataset_ner/devel.txt\n",
      "Uploaded ./dataset_ner/devel.txt, 1 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/labels.txt\n",
      "Uploaded ./dataset_ner/labels.txt, 2 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/test.txt\n",
      "Uploaded ./dataset_ner/test.txt, 3 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/devel.pkl\n",
      "Uploaded ./dataset_ner/devel.pkl, 4 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/train.txt\n",
      "Uploaded ./dataset_ner/train.txt, 5 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/train_dev.txt\n",
      "Uploaded ./dataset_ner/train_dev.txt, 6 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/test.pkl\n",
      "Uploaded ./dataset_ner/test.pkl, 7 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/train.pkl\n",
      "Uploaded ./dataset_ner/train.pkl, 8 files out of an estimated total of 9\n",
      "Uploading ./dataset_ner/train_dev.pkl\n",
      "Uploaded ./dataset_ner/train_dev.pkl, 9 files out of an estimated total of 9\n",
      "Uploaded 9 files\n"
     ]
    }
   ],
   "source": [
    "#Upload processed datasets to datastore\n",
    "data_store = Datastore(ws, \"workspaceblobstore\")\n",
    "data_store.upload(src_dir=\"./dataset_ner\", target_path=\"dataset_ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8de6e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 10 files\n",
      "Target already exists. Skipping upload for dataset_re/dev.tsv\n",
      "Target already exists. Skipping upload for dataset_re/dev_rel.pkl\n",
      "Target already exists. Skipping upload for dataset_re/test.pkl\n",
      "Target already exists. Skipping upload for dataset_re/test.tsv\n",
      "Target already exists. Skipping upload for dataset_re/test_labels.tsv\n",
      "Target already exists. Skipping upload for dataset_re/test_labels_rel.pkl\n",
      "Target already exists. Skipping upload for dataset_re/test_rel.pkl\n",
      "Target already exists. Skipping upload for dataset_re/train.pkl\n",
      "Target already exists. Skipping upload for dataset_re/train.tsv\n",
      "Target already exists. Skipping upload for dataset_re/train_rel.pkl\n",
      "Uploaded 0 files\n"
     ]
    }
   ],
   "source": [
    "#Upload processed datasets to datastore\n",
    "data_store = Datastore(ws, \"workspaceblobstore\")\n",
    "data_store.upload(src_dir=\"./dataset_re\", target_path=\"dataset_re\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff16a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data_store_path = [DataPath(data_store, \"dataset_ner/\")]\n",
    "re_data_store_path = [DataPath(data_store, \"dataset_re/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1c3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create file datasets\n",
    "ner_file_dataset = Dataset.File.from_files(path=ner_data_store_path)\n",
    "re_file_dataset = Dataset.File.from_files(path=re_data_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af66b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register Datasets\n",
    "\n",
    "register_ner = ner_file_dataset.register(workspace=ws,\n",
    "                                    name=\"ehr_ade_labelled_dataset_ner\",\n",
    "                                    description=\"Tokenized EHR and ADE labelled dataset splits for NER task\",\n",
    "                                    tags={\"file_type\": \"directory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f94d4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_re = re_file_dataset.register(workspace=ws,\n",
    "                                    name=\"ehr_ade_labelled_dataset_re\",\n",
    "                                    description=\"Tokenized EHR and ADE labelled dataset splits for RE task\",\n",
    "                                    tags={\"file_type\": \"directory\"})"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
